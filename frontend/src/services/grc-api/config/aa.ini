خطة تنفيذ متقدّمة — مرحلتيْن (1) خط بيانات موحّد وذكي و (2) إدارة الـKnowledge (Vectorization, Indexing & RAG)

جميل — تحت ستجد خطة عملية مفصّلة وقابلة للتنفيذ (هندسة، مكوّنات، تطبيق فعلي، أمثلة كود، قياسات، متطلبات أمان، ونصائح نشر). أخذت‌ بالاعتبار متطلبات مؤسسية: تعدد مستأجرين، امتثال، قابلية قياس، وتقليل تكلفة الاستدلال.

1) خط بيانات موحّد وذكي (Data Ingestion & Normalization Pipeline)
الهدف

تحويل أي مدخل (PDF, DOCX, صور، صفحات ويب، API) إلى مخطط قنوني موحّد (canonical schema) صالح للبحث، التخزين، والفهرسة، مع تتبع provenance وملفّات الميتاداتا.

المعمارية المقترحة (high-level)

Ingesters → Preprocessor (OCR / Extract) → Parser / Splitter → Normalizer (schema mapping) → Enrichment (NER, PII redaction, language detect, metadata) → Store raw + canonical doc + events → Vectorizer/Indexer (RAG pipeline).

مكونات مقترحة:

Entry points: S3/Blob upload, HTTP upload API, Email ingest, Connector (Google Drive, Sharepoint).

Ingestion bus: Kafka / Azure Event Grid / RabbitMQ (events: uploaded, processed, failed).

Orchestrator: Apache NiFi (flow-based), أو Airflow (batch + DAGs)، أو Azure Data Factory (if Azure-first).

OCR & Extract: Azure Form Recognizer / Google Vision / Tesseract (fallback).

Document Parsers: Apache Tika / pdfplumber / python-docx / textract.

Normalization service: microservice that maps fields → canonical JSON.

Storage:

Raw files: object storage (Azure Blob / S3).

Canonical docs & metadata: PostgreSQL (multi-tenant via tenant_id) + optionally Mongo for flexible fields.

Audit & logs: Elastic / Azure Log Analytics.

Queue for downstream: Vectorization workers read canonical docs.

Canonical schema (مثال JSON)
{
  "doc_id": "uuid",
  "tenant_id": "tenant-123",
  "source": "upload|email|sharepoint",
  "original_filename": "rfp-healthcare.pdf",
  "mime_type": "application/pdf",
  "pages": 12,
  "language": "ar",
  "lang_confidence": 0.98,
  "extracted_text": "...",         // full clean text or pointer to blob
  "chunks": [                      // populated after chunking
    {"chunk_id":"cid-1","text":"...", "tokens":1240, "page_start":1, "page_end":2}
  ],
  "metadata": {
    "title":"Tender X",
    "issuer":"Ministry Y",
    "deadline":"2026-02-01",
    "value_est":"SAR 3,000,000"
  },
  "pii_flags": {"has_ssn": false},
  "processing": {
    "ingest_ts":"2025-11-09T10:00Z",
    "ocr_engine":"azure-form-recognizer:v3",
    "status":"processed",
    "version":"v1.2"
  }
}

خطوات التنفيذ التفصيلية
A — نقطة الإدخال (Upload API)

HTTP multipart endpoint /api/upload الذي:

يقبل ملف + metadata + tenant_id + auth token.

يعيد doc_id فوريًا (async processing).

يدفع حدث إلى Kafka/EventGrid.

B — Preprocessing (OCR & Extract)

Workflow:

لو pdf فيه نص → استخدم pdfplumber لاستخراج نص أولي.

إذا صورة أو pdf مسحوب → استعمل Azure Form Recognizer أو Google Vision OCR.

اجمع صفحات كنص منفصل واحتفظ بالنُسخ الثنائية في Blob.

احفظ نتائج OCR الخام + confidence scores.

C — Parsing & Normalizing

استخدم Apache Tika / custom parsers لاستخراج: العناوين، الجداول، القوائم، المراجع.

خوارزمية تطابق الحقول: قوالب (regex) + ML NER (spaCy) لتفكيك issuer/date/values.

ضع mapping rules قابلة للتعديل (YAML) لكل نوع مستند (RFP, Contract, Invoice).

D — Chunking (مهم جداً لتحسين RAG)

Tokenization: استخدم نفس tokeniser للنماذج (tiktoken أو sentencepiece).

قواعد chunking:

chunk_size ≈ 1000–1500 tokens (اعتماداً على النموذج).

overlap ≈ 100–200 tokens (sliding window).

boundary-aware: حاول الانقسام عند فواصل فقرات أو عناوين صفحات.

خزّن لكل chunk: chunk_id, doc_id, page-range, token_count, text_hash.

E — Enrichment

Language detection (fastText/CLD3)

NER / keyphrase extraction (spaCy, transformers)

PII detection/redaction (regex + ML DLP)

Quality metrics: OCR confidence, parse completeness

F — Provenance & Audit

لكل تغيير / خطوة، سجّل record في audit: who, tool, config, timestamp.

خزن صورة الـpipeline config (e.g., ocr_engine: azure:v3, chunking: v1) ضمن processing.version.

G — Error handling & DLQ

أي خطأ يكتب لDead-Letter Queue; مكوّن إعادة المحاولة مع limits؛ انذار تلقائي إلى فريق الدعم.

نشر وتشغيل

Workers: containerized (Docker), autoscale on CPU/GPU need.

Orchestration: Kubernetes (AKS/EKS/On-Prem K8s), Helm charts.

Secrets: Azure Key Vault / HashiCorp Vault.

CI/CD: pipeline يختبر parsers على مجموعة ملفات حقيقية قبل الرفع.

اختبار + قياس الجودة (KPI)

% مستندات مُعالَجة بنجاح (target ≥ 98% للنوعيات المتوقعة)

متوسط وقت المعالجة لكل مستند (target < 10s للملفات الصغيرة، < 60s للملفات الكبيرة)

OCR accuracy (word-level) — مقارنة بعينات مدققة

Coverage of metadata extraction (extracted_title_rate, extracted_deadline_rate)

2) إدارة الـKnowledge (Vectorization, Indexing & RAG)
الهدف

تحويل الـchunks المعيارية إلى vectors، فهرستها مع metadata قوية، وبناء مسار استرجاع (retrieval) ذكي: candidate retrieval → rerank → assemble prompt → LLM call (with provenance).

المعمارية المقترحة (high-level)

Canonical Chunks → Embedding workers → Vector DB (Qdrant) + Metadata DB → Retriever (dense + sparse hybrid) → Reranker (cross-encoder or score via embedding similarity + BM25) → Prompt assembly → Model Router → Response + Save trace.

مكونات مقترحة:

Embeddings: OpenAI / Google embeddings أو محلي (sentence-transformers, onnx) للخصوصية.

Vector DB: Qdrant (self-hosted) أو Pinecone (managed) أو pgvector (Postgres).

Sparse index: Elasticsearch / OpenSearch for BM25 lexical retrieval (good for exact terms).

Retriever orchestration: combine BM25 top-N + dense top-K → merge candidates → rerank via cross-encoder (e.g., miniLM-cross) or rescore using LLM with prompt.

Cache layer: Redis for prompt/result caching.

Prompt assembly: templates + top-k contexts + metadata + instruction skeleton.

Evaluation infra: store human judgments, compute MRR, NDCG.

تصميم الـIndex (schema)

في Qdrant store vector + payload(metadata):

{
  "id":"chunk-uuid",
  "vector":[0.123,...],
  "payload":{
    "doc_id":"doc-uuid",
    "tenant_id":"tenant-123",
    "page_start":1,
    "page_end":2,
    "text_hash":"sha256..",
    "title":"Tender X",
    "source":"rfp",
    "embedding_model":"sbert-paraphrase-v1",
    "created_at":"..."
  }
}

أفضل ممارسات للـEmbeddings

احتفظ بـembedding_model وversion في payload.

استخدم hybrid strategy: local cheap embeddings for initial filtering + expensive cloud embeddings for final ranking (if cost-sensitive).

قم بـperiodic re-embedding عند تغيير model or prompt templates.

Chunking → Embeddings worker (Python example)
# example: chunk -> embed -> upsert to qdrant
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import uuid, hashlib

model = SentenceTransformer("all-mpnet-base-v2")
q = QdrantClient(url="http://qdrant:6333")

def embed_and_upsert(doc_id, tenant_id, chunks):
    vectors = model.encode([c['text'] for c in chunks], show_progress_bar=False)
    points = []
    for c, v in zip(chunks, vectors):
        pid = str(uuid.uuid4())
        payload = {
            "doc_id": doc_id,
            "tenant_id": tenant_id,
            "page_start": c.get("page_start"),
            "page_end": c.get("page_end"),
            "text_hash": hashlib.sha256(c['text'].encode()).hexdigest(),
            "embedding_model": "all-mpnet-base-v2"
        }
        points.append({"id": pid, "vector": v.tolist(), "payload": payload})
    q.upsert("shahin_collection", points)

Retriever: hybrid (BM25 + Dense) strategy

BM25 (ES): get top-50 lexically similar chunks (good for proper nouns, dates).

Dense (Qdrant): top-50 by cosine/similarity.

Union candidates → re-rank:

Use a cross-encoder transformer (e.g., cross-encoder/ms-marco) to compute final relevance scores,

Or compute combined score: alpha * dense_sim + beta * bm25_score + gamma * recency_score.

Return top-K (K=3..8) to assembly.

Reranker example (python)
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tok = AutoTokenizer.from_pretrained("cross-encoder/ms-marco-MiniLM-L-6-v2")
model = AutoModelForSequenceClassification.from_pretrained("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank(query, candidates):
    inputs = tok([query+" </s> "+c["text"] for c in candidates], truncation=True, padding=True, return_tensors="pt")
    scores = model(**inputs).logits.squeeze(-1).detach().cpu().numpy()
    ranked = sorted(zip(candidates, scores), key=lambda x: -x[1])
    return [r[0] for r in ranked]

Prompt assembly & safety

Include: metadata header (doc title, source, date), top-k contexts (with chunk ids), explicit instruction, output schema, max tokens.

Redact any PII flagged earlier or mark fields to never include in prompt.

Limit total input tokens: if contexts exceed budget, use summarizer step: summarize some retrieved contexts and include summaries.

Example prompt skeleton:

[METADATA]
title: {title}
source: {source}
tenant: {tenant_id}
---

[CONTEXTS]
1) {context1_text} (doc:{doc_id}, chunk:{chunk_id})
2) {context2_text} ...

[INSTRUCTION]
As a domain expert, produce: JSON {summary, requirements[], risks[]}. Use ONLY the contexts above. Cite chunk ids in each output item.

Model routing (Model Mesh)

For light tasks (summarize, classification) → cheaper model (smaller LLM or instruction-tuned 7B).

For authoring or drafting sensitive legal text → Gemini-2.5-pro or higher.

Implement policy engine: per-tenant model allowlist, cost cap, fallback rules.

Provenance & saving results

Save: request_hash, prompt_version, retrieved_chunk_ids, model_name, model_tokens_used, model_response, evaluation_score, timestamp.

Store response artifacts in DB with link to audit logs.

Caching & cost-control

Cache responses keyed by sha256(model+prompt_template+topk_hashes+query) in Redis with TTL; use stale-while-revalidate pattern.

Meter model calls per tenant; abort calls if quota exceeded.

Evaluation & KPIs

Human eval process: sample responses → label relevance, correctness. Compute Precision@k, MRR, NDCG.

Operational KPIs:

MRR (target depends on domain; initially >0.4 is good for RFP domain).

Latency: P95 retrieval+rerank < 300ms (dense) + model latency separate.

Cache hit rate > 40% first stage.

Cost per response — set budget.

Monitoring & Observability

Track metrics: embedding time, Qdrant query time, BM25 time, reranker time, total tokens.

Alerts on: embedding worker failure rate > 1%, vector DB disk usage > 70%, increase in latency or drop in MRR.

Backfill, Reindexing & Model upgrades

Maintain jobs to re-embed and reindex full corpus when:

embedding model upgrades, or

schema changes.

Use rolling updates to avoid downtime.

Privacy & Compliance

Tag each vector tenant_id — enforce RBAC queries (only search within tenant scope).

For highly-sensitive tenants offer on-prem / VNet deployment of Qdrant + local embeddings and keep raw files in tenant-controlled storage.

Keep data retention policy and deletion endpoints (right-to-be-forgotten).

Implementation roadmap (concrete sprints)

Sprint 0 (PoC — 1 week)

Build Upload API, simple worker pipeline: pdf->text (pdfplumber) -> chunk -> embed (sentence-transformers) -> Qdrant upsert.

Basic retrieval endpoint that returns top-5 contexts.

Sprint 1 (MVP — 2–3 weeks)

Add OCR fallback (Azure FR), normalization rules for RFPs, metadata extraction, provenance logs.

Add BM25 index (Elasticsearch) and hybrid retriever.

Implement prompt assembly + call to LLM (Gemini proxy). Cache layer + basic UI.

Sprint 2 (Harden & Scale — 3–4 weeks)

Add reranker (cross-encoder).

Multi-tenant enforcement, quotas, KeyVault.

Monitoring + QA pipelines (lhci for UI, integration tests for pipeline).

Backfill scripts & re-embed jobs.

Sprint 3 (Enterprise features)

HITL interface, human evaluation workflows, periodic re-training pipeline (LoRA/fine-tuning), on-prem tenant deployment pack.

أمثلة نهايات API (REST)

POST /api/documents/upload → returns {doc_id}

GET /api/documents/{doc_id} → canonical doc status + metadata

POST /api/embeddings/upsert → bulk upsert chunks

POST /api/search/query → input {query, tenant_id, k} → returns {results:[{chunk_id, score, text, doc_id}]}

POST /api/rag/ask → {query, tenant_id, prompt_template} → returns {answer, used_chunks, model_meta}

أمثلة أتمتة (Airflow DAG skeleton)
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

with DAG('doc_ingest', start_date=datetime(2025,11,1), schedule_interval=None) as dag:
    t1 = PythonOperator(task_id='download_blob', python_callable=download_blob)
    t2 = PythonOperator(task_id='ocr_extract', python_callable=ocr_extract)
    t3 = PythonOperator(task_id='parse_normalize', python_callable=parse_normalize)
    t4 = PythonOperator(task_id='chunk_embed_upsert', python_callable=chunk_embed_upsert)
    t1 >> t2 >> t3 >> t4

نصائح عملية نهائية / قرارات تقنية للتفضيل

Vector DB: ابدأ بـ Qdrant لمرونة self-host + hybrid search، وانتقل لـ Pinecone عند الحاجة لإدارة شدة التحميل.

Embeddings: ابدأ بـ all-mpnet أو all-MiniLM محليًا إن الخصوصية مهمة؛ استخدم Google/OpenAI embeddings إذا تريد أعلى جودة بسرعة.

Retriever: اعمل hybrid (BM25 + dense) — يعطي أفضل نتائج للوثائق القانونية/رقمية.

Reranker: cross-encoder مهم لرفع MRR بشكل ملحوظ.

Provenance: اجعل كل استجابة قابلة للاستفسار: من أي chunk أتى الجواب؟ أي نسخة embedding؟ أي prompt_version استُخدمت؟